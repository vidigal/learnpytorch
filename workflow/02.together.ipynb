{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1+cu117'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import PyTorch and matplotlib\n",
    "import torch\n",
    "from torch import nn # nn contains all of PyTorch's building blocks for neural networks\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "# Check PyTorch version\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Setup device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000],\n",
       "         [0.0200],\n",
       "         [0.0400],\n",
       "         [0.0600],\n",
       "         [0.0800],\n",
       "         [0.1000],\n",
       "         [0.1200],\n",
       "         [0.1400],\n",
       "         [0.1600],\n",
       "         [0.1800]]),\n",
       " tensor([[0.3000],\n",
       "         [0.3140],\n",
       "         [0.3280],\n",
       "         [0.3420],\n",
       "         [0.3560],\n",
       "         [0.3700],\n",
       "         [0.3840],\n",
       "         [0.3980],\n",
       "         [0.4120],\n",
       "         [0.4260]]))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create weight and bias\n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "\n",
    "# Create range values\n",
    "start = 0\n",
    "end = 1000000\n",
    "step = 0.02\n",
    "\n",
    "# Create X and y (features and labels)\n",
    "X = torch.arange(start, end, step).unsqueeze(dim=1) # without unsqueeze, errors will happen later on (shapes within linear layers)\n",
    "y = weight * X + bias \n",
    "X[:10], y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000000, 40000000, 10000000, 10000000)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data\n",
    "train_split = int(0.8 * len(X))\n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "X_test, y_test = X[train_split:], y[train_split:]\n",
    "\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(train_data=X_train, \n",
    "                     train_labels=y_train, \n",
    "                     test_data=X_test, \n",
    "                     test_labels=y_test, \n",
    "                     predictions=None):\n",
    "    plt.figure(figsize=(5, 3.5))\n",
    "    plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Dados de treinamento\")\n",
    "    plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Dados de teste\")\n",
    "    if predictions is not None:\n",
    "        # Plot the predictions in red (predictions were made on the test data)\n",
    "        plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
    "\n",
    "\n",
    "    plt.legend(prop={\"size\":14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: If you've reset your runtime, this function won't work, \n",
    "# you'll have to rerun the cell above where it's instantiated.\n",
    "plot_predictions(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclass nn.Module to make our model\n",
    "class LinearRegressionModelV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Use nn.Linear() for creating the model parameters\n",
    "        self.linear_layer = nn.Linear(in_features=1, \n",
    "                                      out_features=1)\n",
    "    \n",
    "    # Define the forward computation (input data x flows through nn.Linear())\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear_layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LinearRegressionModelV2(\n",
       "   (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n",
       " ),\n",
       " OrderedDict([('linear_layer.weight', tensor([[0.7645]])),\n",
       "              ('linear_layer.bias', tensor([0.8300]))]))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the manual seed when creating the model (this isn't always need but is used for demonstrative purposes, try commenting it out and seeing what happens)\n",
    "torch.manual_seed(42)\n",
    "model_1 = LinearRegressionModelV2()\n",
    "model_1, model_1.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model device\n",
    "next(model_1.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to GPU if it's availalble, otherwise it'll default to CPU\n",
    "model_1.to(device) # the device variable was set above to be \"cuda\" if available or \"cpu\" if not\n",
    "next(model_1.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss function\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.AdamW(params=model_1.parameters(), # optimize newly created model's parameters\n",
    "                            lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Set the number of epochs \n",
    "epochs = 1000000 \n",
    "\n",
    "# Put data on the available device\n",
    "# Without this, error will happen (not all model/data on device)\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model_1.train() # train mode is on by default after construction\n",
    "\n",
    "    # 1. Forward pass\n",
    "    y_pred = model_1(X_train)\n",
    "\n",
    "    # 2. Calculate loss\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "    # 3. Zero grad optimizer\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backward\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Step the optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model_1.eval() # put the model in evaluation mode for testing (inference)\n",
    "    # 1. Forward pass\n",
    "    with torch.inference_mode():\n",
    "        test_pred = model_1(X_test)\n",
    "    \n",
    "        # 2. Calculate the loss\n",
    "        test_loss = loss_fn(test_pred, y_test)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find our model's learned parameters\n",
    "from pprint import pprint # pprint = pretty print, see: https://docs.python.org/3/library/pprint.html \n",
    "print(\"The model learned the following values for weights and bias:\")\n",
    "pprint(model_1.state_dict())\n",
    "print(\"\\nAnd the original values for weights and bias are:\")\n",
    "print(f\"weights: {weight}, bias: {bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn model into evaluation mode\n",
    "model_1.eval()\n",
    "\n",
    "# Make predictions on the test data\n",
    "with torch.inference_mode():\n",
    "    y_preds = model_1(torch.tensor([[10]], device=\"cuda\", dtype=torch.float32).T)\n",
    "y_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_predictions(predictions=y_preds) # -> won't work... data not on CPU\n",
    "\n",
    "# Put data on the CPU and plot it\n",
    "plot_predictions(predictions=y_preds.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "MODEL_PATH = Path(\"../models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"my_model_v01.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Salvando modelo: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=loaded_model_1.state_dict(), f=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando modelo salvo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegressionModelV2(\n",
       "  (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model_1 = LinearRegressionModelV2()\n",
    "loaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "loaded_model_1.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss function\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.Adam(params=loaded_model_1.parameters(), # optimize newly created model's parameters\n",
    "                            lr=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train loss: 0.007823426276445389 | Test loss: 0.8980956077575684\n",
      "Epoch: 100 | Train loss: 0.00782475620508194 | Test loss: 0.040073562413454056\n",
      "Epoch: 200 | Train loss: 0.017352791503071785 | Test loss: 0.012500000186264515\n",
      "Epoch: 300 | Train loss: 0.017339637503027916 | Test loss: 0.012500000186264515\n",
      "Epoch: 400 | Train loss: 0.017326543107628822 | Test loss: 0.012500000186264515\n",
      "Epoch: 500 | Train loss: 0.017314566299319267 | Test loss: 0.012500000186264515\n",
      "Epoch: 600 | Train loss: 0.007838912308216095 | Test loss: 0.012500000186264515\n",
      "Epoch: 700 | Train loss: 0.05403891205787659 | Test loss: 0.04002825543284416\n",
      "Epoch: 800 | Train loss: 0.017286796122789383 | Test loss: 0.0932701826095581\n",
      "Epoch: 900 | Train loss: 0.007841438986361027 | Test loss: 0.012500000186264515\n",
      "Epoch: 1000 | Train loss: 0.007842247374355793 | Test loss: 0.012500000186264515\n",
      "Epoch: 1100 | Train loss: 0.01724867895245552 | Test loss: 0.012500000186264515\n",
      "Epoch: 1200 | Train loss: 0.007843988947570324 | Test loss: 0.03999576345086098\n",
      "Epoch: 1300 | Train loss: 0.041235581040382385 | Test loss: 0.012500000186264515\n",
      "Epoch: 1400 | Train loss: 0.007845834828913212 | Test loss: 0.012500000186264515\n",
      "Epoch: 1500 | Train loss: 0.01720081828534603 | Test loss: 0.012500000186264515\n",
      "Epoch: 1600 | Train loss: 0.007847650907933712 | Test loss: 0.06780828535556793\n",
      "Epoch: 1700 | Train loss: 0.007848724722862244 | Test loss: 0.012500000186264515\n",
      "Epoch: 1800 | Train loss: 0.017167402431368828 | Test loss: 0.012500000186264515\n",
      "Epoch: 1900 | Train loss: 0.01715490035712719 | Test loss: 0.012500000186264515\n",
      "Epoch: 2000 | Train loss: 0.01714273728430271 | Test loss: 0.0931200236082077\n",
      "Epoch: 2100 | Train loss: 0.007840956561267376 | Test loss: 0.06785272806882858\n",
      "Epoch: 2200 | Train loss: 0.0411430187523365 | Test loss: 0.012500000186264515\n",
      "Epoch: 2300 | Train loss: 0.007835679687559605 | Test loss: 0.012500000186264515\n",
      "Epoch: 2400 | Train loss: 0.007832562550902367 | Test loss: 0.039914101362228394\n",
      "Epoch: 2500 | Train loss: 0.007829447276890278 | Test loss: 0.06788879632949829\n",
      "Epoch: 2600 | Train loss: 0.05424443259835243 | Test loss: 0.12133689969778061\n",
      "Epoch: 2700 | Train loss: 0.007824109867215157 | Test loss: 0.039894357323646545\n",
      "Epoch: 2800 | Train loss: 0.007821738719940186 | Test loss: 0.012500000186264515\n",
      "Epoch: 2900 | Train loss: 0.007818911224603653 | Test loss: 0.012500000186264515\n",
      "Epoch: 3000 | Train loss: 0.017036965116858482 | Test loss: 0.012500000186264515\n",
      "Epoch: 3100 | Train loss: 0.01702730916440487 | Test loss: 0.06793614476919174\n",
      "Epoch: 3200 | Train loss: 0.030393650755286217 | Test loss: 0.039864569902420044\n",
      "Epoch: 3300 | Train loss: 0.01700647920370102 | Test loss: 0.012500000186264515\n",
      "Epoch: 3400 | Train loss: 0.0078046140260994434 | Test loss: 0.03985249996185303\n",
      "Epoch: 3500 | Train loss: 0.007802011910825968 | Test loss: 0.03984718769788742\n",
      "Epoch: 3600 | Train loss: 0.00779930641874671 | Test loss: 0.012500000186264515\n",
      "Epoch: 3700 | Train loss: 0.007796920370310545 | Test loss: 0.012500000186264515\n",
      "Epoch: 3800 | Train loss: 0.016963554546236992 | Test loss: 0.12145888805389404\n",
      "Epoch: 3900 | Train loss: 0.007792713586241007 | Test loss: 0.039827220141887665\n",
      "Epoch: 4000 | Train loss: 0.0077905720099806786 | Test loss: 0.06800105422735214\n",
      "Epoch: 4100 | Train loss: 0.016938384622335434 | Test loss: 0.1214866042137146\n",
      "Epoch: 4200 | Train loss: 0.01693015545606613 | Test loss: 0.06801419705152512\n",
      "Epoch: 4300 | Train loss: 0.007786176633089781 | Test loss: 0.012500000186264515\n",
      "Epoch: 4400 | Train loss: 0.016914477571845055 | Test loss: 0.12151290476322174\n",
      "Epoch: 4500 | Train loss: 0.0077927736565470695 | Test loss: 0.039797455072402954\n",
      "Epoch: 4600 | Train loss: 0.007797006983309984 | Test loss: 0.03979143872857094\n",
      "Epoch: 4700 | Train loss: 0.05442473292350769 | Test loss: 0.12154150009155273\n",
      "Epoch: 4800 | Train loss: 0.007803818676620722 | Test loss: 0.06805417686700821\n",
      "Epoch: 4900 | Train loss: 0.01687299646437168 | Test loss: 0.012500000186264515\n",
      "Epoch: 5000 | Train loss: 0.007810346316546202 | Test loss: 0.03977213054895401\n",
      "Epoch: 5100 | Train loss: 0.00781357754021883 | Test loss: 0.0680728480219841\n",
      "Epoch: 5200 | Train loss: 0.03057296946644783 | Test loss: 0.03976316750049591\n",
      "Epoch: 5300 | Train loss: 0.007819116115570068 | Test loss: 0.06808381527662277\n",
      "Epoch: 5400 | Train loss: 0.007821663282811642 | Test loss: 0.039755161851644516\n",
      "Epoch: 5500 | Train loss: 0.016831394284963608 | Test loss: 0.012500000186264515\n",
      "Epoch: 5600 | Train loss: 0.03060055337846279 | Test loss: 0.09278157353401184\n",
      "Epoch: 5700 | Train loss: 0.04086276516318321 | Test loss: 0.09277511388063431\n",
      "Epoch: 5800 | Train loss: 0.007829970680177212 | Test loss: 0.06810565292835236\n",
      "Epoch: 5900 | Train loss: 0.06472296267747879 | Test loss: 0.03974103927612305\n",
      "Epoch: 6000 | Train loss: 0.007833106443285942 | Test loss: 0.03973761200904846\n",
      "Epoch: 6100 | Train loss: 0.00783687923103571 | Test loss: 0.012500000186264515\n",
      "Epoch: 6200 | Train loss: 0.007840720005333424 | Test loss: 0.0397256575524807\n",
      "Epoch: 6300 | Train loss: 0.007844597101211548 | Test loss: 0.01250413153320551\n",
      "Epoch: 6400 | Train loss: 0.016768336296081543 | Test loss: 0.012528494000434875\n",
      "Epoch: 6500 | Train loss: 0.007852448150515556 | Test loss: 0.01255277544260025\n",
      "Epoch: 6600 | Train loss: 0.01674731820821762 | Test loss: 0.012577762827277184\n",
      "Epoch: 6700 | Train loss: 0.00786079652607441 | Test loss: 0.03969418257474899\n",
      "Epoch: 6800 | Train loss: 0.007864675484597683 | Test loss: 0.012628731317818165\n",
      "Epoch: 6900 | Train loss: 0.007868370041251183 | Test loss: 0.03968143090605736\n",
      "Epoch: 7000 | Train loss: 0.007872020825743675 | Test loss: 0.012677456252276897\n",
      "Epoch: 7100 | Train loss: 0.03073211945593357 | Test loss: 0.03966941311955452\n",
      "Epoch: 7200 | Train loss: 0.007875978946685791 | Test loss: 0.012704368680715561\n",
      "Epoch: 7300 | Train loss: 0.016688909381628036 | Test loss: 0.012722006067633629\n",
      "Epoch: 7400 | Train loss: 0.016682587563991547 | Test loss: 0.06821545958518982\n",
      "Epoch: 7500 | Train loss: 0.007883456535637379 | Test loss: 0.06822089105844498\n",
      "Epoch: 7600 | Train loss: 0.007885750383138657 | Test loss: 0.012771912850439548\n",
      "Epoch: 7700 | Train loss: 0.00788814015686512 | Test loss: 0.012788519263267517\n",
      "Epoch: 7800 | Train loss: 0.030783679336309433 | Test loss: 0.012806006707251072\n",
      "Epoch: 7900 | Train loss: 0.030790910124778748 | Test loss: 0.012822712771594524\n",
      "Epoch: 8000 | Train loss: 0.04069849103689194 | Test loss: 0.012838874943554401\n",
      "Epoch: 8100 | Train loss: 0.007897435687482357 | Test loss: 0.01285426877439022\n",
      "Epoch: 8200 | Train loss: 0.007899563759565353 | Test loss: 0.012869887985289097\n",
      "Epoch: 8300 | Train loss: 0.007902084849774837 | Test loss: 0.01288851909339428\n",
      "Epoch: 8400 | Train loss: 0.007904971949756145 | Test loss: 0.03955761343240738\n",
      "Epoch: 8500 | Train loss: 0.016620289534330368 | Test loss: 0.012926424853503704\n",
      "Epoch: 8600 | Train loss: 0.03084263764321804 | Test loss: 0.012941350229084492\n",
      "Epoch: 8700 | Train loss: 0.040652818977832794 | Test loss: 0.012957537546753883\n",
      "Epoch: 8800 | Train loss: 0.01660436950623989 | Test loss: 0.012974306009709835\n",
      "Epoch: 8900 | Train loss: 0.01659868098795414 | Test loss: 0.012991469353437424\n",
      "Epoch: 9000 | Train loss: 0.016592761501669884 | Test loss: 0.013009262271225452\n",
      "Epoch: 9100 | Train loss: 0.01658564619719982 | Test loss: 0.013030762784183025\n",
      "Epoch: 9200 | Train loss: 0.01657826267182827 | Test loss: 0.01305288728326559\n",
      "Epoch: 9300 | Train loss: 0.016570841893553734 | Test loss: 0.013075100257992744\n",
      "Epoch: 9400 | Train loss: 0.01656339131295681 | Test loss: 0.013097512535750866\n",
      "Epoch: 9500 | Train loss: 0.04059170186519623 | Test loss: 0.03945382684469223\n",
      "Epoch: 9600 | Train loss: 0.016553284600377083 | Test loss: 0.06834614276885986\n",
      "Epoch: 9700 | Train loss: 0.04058181121945381 | Test loss: 0.013140825554728508\n",
      "Epoch: 9800 | Train loss: 0.016544394195079803 | Test loss: 0.013154818676412106\n",
      "Epoch: 9900 | Train loss: 0.007949578575789928 | Test loss: 0.03942691162228584\n",
      "Epoch: 10000 | Train loss: 0.00795422401279211 | Test loss: 0.039417218416929245\n",
      "Epoch: 10100 | Train loss: 0.007958543486893177 | Test loss: 0.039408162236213684\n",
      "Epoch: 10200 | Train loss: 0.007962384261190891 | Test loss: 0.01322156935930252\n",
      "Epoch: 10300 | Train loss: 0.054822564125061035 | Test loss: 0.12199912965297699\n",
      "Epoch: 10400 | Train loss: 0.040539056062698364 | Test loss: 0.03938513249158859\n",
      "Epoch: 10500 | Train loss: 0.007973325438797474 | Test loss: 0.013267355971038342\n",
      "Epoch: 10600 | Train loss: 0.030992625281214714 | Test loss: 0.03936981409788132\n",
      "Epoch: 10700 | Train loss: 0.030999334529042244 | Test loss: 0.03936216980218887\n",
      "Epoch: 10800 | Train loss: 0.007984026335179806 | Test loss: 0.013312081806361675\n",
      "Epoch: 10900 | Train loss: 0.016489040106534958 | Test loss: 0.013323218561708927\n",
      "Epoch: 11000 | Train loss: 0.007992858998477459 | Test loss: 0.039339251816272736\n",
      "Epoch: 11100 | Train loss: 0.007998487912118435 | Test loss: 0.03932959958910942\n",
      "Epoch: 11200 | Train loss: 0.008002356626093388 | Test loss: 0.039323002099990845\n",
      "Epoch: 11300 | Train loss: 0.008007756434381008 | Test loss: 0.03931363672018051\n",
      "Epoch: 11400 | Train loss: 0.008013117127120495 | Test loss: 0.0134111437946558\n",
      "Epoch: 11500 | Train loss: 0.01645517535507679 | Test loss: 0.03929658979177475\n",
      "Epoch: 11600 | Train loss: 0.016450658440589905 | Test loss: 0.013440375216305256\n",
      "Epoch: 11700 | Train loss: 0.00802550558000803 | Test loss: 0.06845582276582718\n",
      "Epoch: 11800 | Train loss: 0.016442250460386276 | Test loss: 0.09230011701583862\n",
      "Epoch: 11900 | Train loss: 0.016437962651252747 | Test loss: 0.06846468150615692\n",
      "Epoch: 12000 | Train loss: 0.008036528714001179 | Test loss: 0.03926362097263336\n",
      "Epoch: 12100 | Train loss: 0.008040846325457096 | Test loss: 0.03925598785281181\n",
      "Epoch: 12200 | Train loss: 0.008045123890042305 | Test loss: 0.03924847021698952\n",
      "Epoch: 12300 | Train loss: 0.016419583931565285 | Test loss: 0.0392402820289135\n",
      "Epoch: 12400 | Train loss: 0.01641431823372841 | Test loss: 0.013554694131016731\n",
      "Epoch: 12500 | Train loss: 0.008058646693825722 | Test loss: 0.01356914360076189\n",
      "Epoch: 12600 | Train loss: 0.008062873966991901 | Test loss: 0.013583306223154068\n",
      "Epoch: 12700 | Train loss: 0.008065988309681416 | Test loss: 0.01359382551163435\n",
      "Epoch: 12800 | Train loss: 0.008070318028330803 | Test loss: 0.01360852550715208\n",
      "Epoch: 12900 | Train loss: 0.00807400792837143 | Test loss: 0.039197999984025955\n",
      "Epoch: 13000 | Train loss: 0.008077173493802547 | Test loss: 0.03919229283928871\n",
      "Epoch: 13100 | Train loss: 0.016385208815336227 | Test loss: 0.013646206818521023\n",
      "Epoch: 13200 | Train loss: 0.008084714412689209 | Test loss: 0.013660437427461147\n",
      "Epoch: 13300 | Train loss: 0.016375649720430374 | Test loss: 0.013676425442099571\n",
      "Epoch: 13400 | Train loss: 0.016372403129935265 | Test loss: 0.01368663739413023\n",
      "Epoch: 13500 | Train loss: 0.008096388541162014 | Test loss: 0.03915715590119362\n",
      "Epoch: 13600 | Train loss: 0.016364041715860367 | Test loss: 0.013713168911635876\n",
      "Epoch: 13700 | Train loss: 0.04035573825240135 | Test loss: 0.013725687749683857\n",
      "Epoch: 13800 | Train loss: 0.016356198117136955 | Test loss: 0.09218344837427139\n",
      "Epoch: 13900 | Train loss: 0.008109904825687408 | Test loss: 0.01375245675444603\n",
      "Epoch: 14000 | Train loss: 0.008112644776701927 | Test loss: 0.039126913994550705\n",
      "Epoch: 14100 | Train loss: 0.008117038756608963 | Test loss: 0.03911863639950752\n",
      "Epoch: 14200 | Train loss: 0.008119994774460793 | Test loss: 0.03911321982741356\n",
      "Epoch: 14300 | Train loss: 0.008123152889311314 | Test loss: 0.01380151230841875\n",
      "Epoch: 14400 | Train loss: 0.055046770721673965 | Test loss: 0.03910013288259506\n",
      "Epoch: 14500 | Train loss: 0.00813062209635973 | Test loss: 0.09215289354324341\n",
      "Epoch: 14600 | Train loss: 0.008134090341627598 | Test loss: 0.013842100277543068\n",
      "Epoch: 14700 | Train loss: 0.01631925255060196 | Test loss: 0.013853856362402439\n",
      "Epoch: 14800 | Train loss: 0.016315478831529617 | Test loss: 0.01386561244726181\n",
      "Epoch: 14900 | Train loss: 0.008144337683916092 | Test loss: 0.092135488986969\n",
      "Epoch: 15000 | Train loss: 0.016306785866618156 | Test loss: 0.039061207324266434\n",
      "Epoch: 15100 | Train loss: 0.008150815963745117 | Test loss: 0.0139055997133255\n",
      "Epoch: 15200 | Train loss: 0.008153822273015976 | Test loss: 0.03904886171221733\n",
      "Epoch: 15300 | Train loss: 0.00815705768764019 | Test loss: 0.03904268890619278\n",
      "Epoch: 15400 | Train loss: 0.055096499621868134 | Test loss: 0.12232517451047897\n",
      "Epoch: 15500 | Train loss: 0.016285063698887825 | Test loss: 0.013958056457340717\n",
      "Epoch: 15600 | Train loss: 0.03129364550113678 | Test loss: 0.013973275199532509\n",
      "Epoch: 15700 | Train loss: 0.008172055706381798 | Test loss: 0.03901321813464165\n",
      "Epoch: 15800 | Train loss: 0.016268566250801086 | Test loss: 0.01400761865079403\n",
      "Epoch: 15900 | Train loss: 0.008181729353964329 | Test loss: 0.038994088768959045\n",
      "Epoch: 16000 | Train loss: 0.008185520768165588 | Test loss: 0.038986511528491974\n",
      "Epoch: 16100 | Train loss: 0.016252582892775536 | Test loss: 0.014055262319743633\n",
      "Epoch: 16200 | Train loss: 0.01624765805900097 | Test loss: 0.014069800265133381\n",
      "Epoch: 16300 | Train loss: 0.008196779526770115 | Test loss: 0.01408410631120205\n",
      "Epoch: 16400 | Train loss: 0.008200595155358315 | Test loss: 0.03895731270313263\n",
      "Epoch: 16500 | Train loss: 0.008204376325011253 | Test loss: 0.038950130343437195\n",
      "Epoch: 16600 | Train loss: 0.016228431835770607 | Test loss: 0.014126419089734554\n",
      "Epoch: 16700 | Train loss: 0.008212038315832615 | Test loss: 0.014140106737613678\n",
      "Epoch: 16800 | Train loss: 0.00821587722748518 | Test loss: 0.03892926126718521\n",
      "Epoch: 16900 | Train loss: 0.016214357689023018 | Test loss: 0.014167875051498413\n",
      "Epoch: 17000 | Train loss: 0.008223802782595158 | Test loss: 0.03891487047076225\n",
      "Epoch: 17100 | Train loss: 0.016204699873924255 | Test loss: 0.01419641263782978\n",
      "Epoch: 17200 | Train loss: 0.008232326246798038 | Test loss: 0.014210406690835953\n",
      "Epoch: 17300 | Train loss: 0.01619473285973072 | Test loss: 0.014225875027477741\n",
      "Epoch: 17400 | Train loss: 0.016189629212021828 | Test loss: 0.014241194352507591\n",
      "Epoch: 17500 | Train loss: 0.008246968500316143 | Test loss: 0.014258594252169132\n",
      "Epoch: 17600 | Train loss: 0.008252409286797047 | Test loss: 0.03886731341481209\n",
      "Epoch: 17700 | Train loss: 0.016171826049685478 | Test loss: 0.014294719323515892\n",
      "Epoch: 17800 | Train loss: 0.00826331228017807 | Test loss: 0.014312737621366978\n",
      "Epoch: 17900 | Train loss: 0.008268806152045727 | Test loss: 0.03883998841047287\n",
      "Epoch: 18000 | Train loss: 0.016153857111930847 | Test loss: 0.014348856173455715\n",
      "Epoch: 18100 | Train loss: 0.00827952940016985 | Test loss: 0.014366875402629375\n",
      "Epoch: 18200 | Train loss: 0.00828484632074833 | Test loss: 0.038812629878520966\n",
      "Epoch: 18300 | Train loss: 0.016135869547724724 | Test loss: 0.014403000473976135\n",
      "Epoch: 18400 | Train loss: 0.008295441046357155 | Test loss: 0.014421018771827221\n",
      "Epoch: 18500 | Train loss: 0.008300736546516418 | Test loss: 0.03878528252243996\n",
      "Epoch: 18600 | Train loss: 0.01611802913248539 | Test loss: 0.014457137323915958\n",
      "Epoch: 18700 | Train loss: 0.008311206474900246 | Test loss: 0.014475156553089619\n",
      "Epoch: 18800 | Train loss: 0.00831641349941492 | Test loss: 0.03875796124339104\n",
      "Epoch: 18900 | Train loss: 0.016102300956845284 | Test loss: 0.014511281624436378\n",
      "Epoch: 19000 | Train loss: 0.008326627314090729 | Test loss: 0.01452882494777441\n",
      "Epoch: 19100 | Train loss: 0.00833103433251381 | Test loss: 0.03873215615749359\n",
      "Epoch: 19200 | Train loss: 0.01608906127512455 | Test loss: 0.014559762552380562\n",
      "Epoch: 19300 | Train loss: 0.008339940570294857 | Test loss: 0.014575231820344925\n",
      "Epoch: 19400 | Train loss: 0.008344496600329876 | Test loss: 0.038708705455064774\n",
      "Epoch: 19500 | Train loss: 0.016076643019914627 | Test loss: 0.014605331234633923\n",
      "Epoch: 19600 | Train loss: 0.008352666161954403 | Test loss: 0.014618262648582458\n",
      "Epoch: 19700 | Train loss: 0.008356275036931038 | Test loss: 0.03868832066655159\n",
      "Epoch: 19800 | Train loss: 0.016066070646047592 | Test loss: 0.014643999747931957\n",
      "Epoch: 19900 | Train loss: 0.008363435976207256 | Test loss: 0.014656906016170979\n",
      "Epoch: 20000 | Train loss: 0.00836701225489378 | Test loss: 0.038668811321258545\n",
      "Epoch: 20100 | Train loss: 0.01605559140443802 | Test loss: 0.014682674780488014\n",
      "Epoch: 20200 | Train loss: 0.00837415736168623 | Test loss: 0.01469560619443655\n",
      "Epoch: 20300 | Train loss: 0.008377705700695515 | Test loss: 0.03864924982190132\n",
      "Epoch: 20400 | Train loss: 0.01604512520134449 | Test loss: 0.014721344225108624\n",
      "Epoch: 20500 | Train loss: 0.00838473904877901 | Test loss: 0.014734250493347645\n",
      "Epoch: 20600 | Train loss: 0.008388265036046505 | Test loss: 0.038629718124866486\n",
      "Epoch: 20700 | Train loss: 0.016034696251153946 | Test loss: 0.01476001925766468\n",
      "Epoch: 20800 | Train loss: 0.008395242504775524 | Test loss: 0.014772949740290642\n",
      "Epoch: 20900 | Train loss: 0.00839872658252716 | Test loss: 0.03861018642783165\n",
      "Epoch: 21000 | Train loss: 0.01602417230606079 | Test loss: 0.014798687770962715\n",
      "Epoch: 21100 | Train loss: 0.008405662141740322 | Test loss: 0.014811594039201736\n",
      "Epoch: 21200 | Train loss: 0.008408666588366032 | Test loss: 0.03859160840511322\n",
      "Epoch: 21300 | Train loss: 0.01601484790444374 | Test loss: 0.014832869172096252\n",
      "Epoch: 21400 | Train loss: 0.008414262905716896 | Test loss: 0.014843150041997433\n",
      "Epoch: 21500 | Train loss: 0.008416994474828243 | Test loss: 0.03857598826289177\n",
      "Epoch: 21600 | Train loss: 0.016006411984562874 | Test loss: 0.014863806776702404\n",
      "Epoch: 21700 | Train loss: 0.00842246226966381 | Test loss: 0.014874087646603584\n",
      "Epoch: 21800 | Train loss: 0.008425175212323666 | Test loss: 0.03856035694479942\n",
      "Epoch: 21900 | Train loss: 0.015997953712940216 | Test loss: 0.014894744381308556\n",
      "Epoch: 22000 | Train loss: 0.008430593647062778 | Test loss: 0.014905025251209736\n",
      "Epoch: 22100 | Train loss: 0.008433286100625992 | Test loss: 0.03854473680257797\n",
      "Epoch: 22200 | Train loss: 0.015989558771252632 | Test loss: 0.014925681054592133\n",
      "Epoch: 22300 | Train loss: 0.008438678458333015 | Test loss: 0.014935962855815887\n",
      "Epoch: 22400 | Train loss: 0.008441382087767124 | Test loss: 0.038529105484485626\n",
      "Epoch: 22500 | Train loss: 0.015981189906597137 | Test loss: 0.014956618659198284\n",
      "Epoch: 22600 | Train loss: 0.008446718566119671 | Test loss: 0.014966900460422039\n",
      "Epoch: 22700 | Train loss: 0.008449372835457325 | Test loss: 0.038513489067554474\n",
      "Epoch: 22800 | Train loss: 0.01597282849252224 | Test loss: 0.014987556263804436\n",
      "Epoch: 22900 | Train loss: 0.008454679511487484 | Test loss: 0.01499783806502819\n",
      "Epoch: 23000 | Train loss: 0.008457314223051071 | Test loss: 0.03849785774946213\n",
      "Epoch: 23100 | Train loss: 0.015964491292834282 | Test loss: 0.015018493868410587\n",
      "Epoch: 23200 | Train loss: 0.008462573401629925 | Test loss: 0.015028774738311768\n",
      "Epoch: 23300 | Train loss: 0.008465186692774296 | Test loss: 0.03848223760724068\n",
      "Epoch: 23400 | Train loss: 0.015956183895468712 | Test loss: 0.015049431473016739\n",
      "Epoch: 23500 | Train loss: 0.008470877073705196 | Test loss: 0.01505971234291792\n",
      "Epoch: 23600 | Train loss: 0.008474155329167843 | Test loss: 0.03846660628914833\n",
      "Epoch: 23700 | Train loss: 0.015947649255394936 | Test loss: 0.01508036907762289\n",
      "Epoch: 23800 | Train loss: 0.00848081149160862 | Test loss: 0.01509064994752407\n",
      "Epoch: 23900 | Train loss: 0.008484194986522198 | Test loss: 0.03845098614692688\n",
      "Epoch: 24000 | Train loss: 0.015939097851514816 | Test loss: 0.015111306682229042\n",
      "Epoch: 24100 | Train loss: 0.008490963838994503 | Test loss: 0.015121587552130222\n",
      "Epoch: 24200 | Train loss: 0.008494448848068714 | Test loss: 0.03843535855412483\n",
      "Epoch: 24300 | Train loss: 0.015930509194731712 | Test loss: 0.015142244286835194\n",
      "Epoch: 24400 | Train loss: 0.008501500822603703 | Test loss: 0.015152525156736374\n",
      "Epoch: 24500 | Train loss: 0.00850500725209713 | Test loss: 0.03841973841190338\n",
      "Epoch: 24600 | Train loss: 0.015921935439109802 | Test loss: 0.015173181891441345\n",
      "Epoch: 24700 | Train loss: 0.008511573076248169 | Test loss: 0.01518215611577034\n",
      "Epoch: 24800 | Train loss: 0.008513842709362507 | Test loss: 0.038406699895858765\n",
      "Epoch: 24900 | Train loss: 0.015916144475340843 | Test loss: 0.015193913131952286\n",
      "Epoch: 25000 | Train loss: 0.008517417125403881 | Test loss: 0.015199094079434872\n",
      "Epoch: 25100 | Train loss: 0.008519330993294716 | Test loss: 0.0383988618850708\n",
      "Epoch: 25200 | Train loss: 0.01591183990240097 | Test loss: 0.015209381468594074\n",
      "Epoch: 25300 | Train loss: 0.008523127995431423 | Test loss: 0.01521456241607666\n",
      "Epoch: 25400 | Train loss: 0.008525037206709385 | Test loss: 0.03839104250073433\n",
      "Epoch: 25500 | Train loss: 0.015907524153590202 | Test loss: 0.015224849805235863\n",
      "Epoch: 25600 | Train loss: 0.008528823964297771 | Test loss: 0.015230031684041023\n",
      "Epoch: 25700 | Train loss: 0.05561218783259392 | Test loss: 0.01523144356906414\n",
      "Epoch: 25800 | Train loss: 0.0159047469496727 | Test loss: 0.015234818682074547\n",
      "Epoch: 25900 | Train loss: 0.008533419109880924 | Test loss: 0.015242581255733967\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m     test_loss \u001b[39m=\u001b[39m loss_fn(test_pred, y_test)\n\u001b[1;32m     41\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m | Train loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m | Test loss: \u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:872\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39m\u001b[39m__format__\u001b[39m, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, format_spec)\n\u001b[1;32m    871\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_meta \u001b[39mand\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39mis\u001b[39;00m Tensor:\n\u001b[0;32m--> 872\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mitem()\u001b[39m.\u001b[39m\u001b[39m__format__\u001b[39m(format_spec)\n\u001b[1;32m    873\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__format__\u001b[39m(\u001b[39mself\u001b[39m, format_spec)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Set the number of epochs \n",
    "epochs = 1000000 \n",
    "\n",
    "# Put data on the available device\n",
    "# Without this, error will happen (not all model/data on device)\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    loaded_model_1.train() # train mode is on by default after construction\n",
    "\n",
    "    # 1. Forward pass\n",
    "    y_pred = loaded_model_1(X_train)\n",
    "\n",
    "    # 2. Calculate loss\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "    # 3. Zero grad optimizer\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backward\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Step the optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    loaded_model_1.eval() # put the model in evaluation mode for testing (inference)\n",
    "    # 1. Forward pass\n",
    "    with torch.inference_mode():\n",
    "        test_pred = loaded_model_1(X_test)\n",
    "    \n",
    "        # 2. Calculate the loss\n",
    "        test_loss = loss_fn(test_pred, y_test)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.2984]], device='cuda:0')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn model into evaluation mode\n",
    "loaded_model_1.eval()\n",
    "\n",
    "# Make predictions on the test data\n",
    "with torch.inference_mode():\n",
    "    y_preds = loaded_model_1(torch.tensor([[10]], device=\"cuda\", dtype=torch.float32).T)\n",
    "y_preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnpytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
